{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPtFBgj623FS"
   },
   "source": [
    "# Accessing HF Inference Endpoints Like a Developer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w4egfB274VD"
   },
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "The first thing we'll do is load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23H7TMOM4mfy",
    "outputId": "afea5022-eb08-4881-a769-caf445e0a267"
   },
   "outputs": [],
   "source": [
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKD8XBTVEAOw"
   },
   "source": [
    "## 2. Setting Environment Variables\n",
    "\n",
    "As we'll frequently use various endpoints and APIs hosted by others - we'll need to handle our \"secrets\" or API keys very often.\n",
    "\n",
    "We'll use the following pattern throughout this workshop - but you can use whichever method you're most familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGU9OMvhEPG0",
    "outputId": "1375a021-5998-4874-b286-825ccbc6877b"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HuggingFace API Key ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7WqAy8lu-fR"
   },
   "source": [
    "We'll also need to provide your Hugging Face Inference Endpoint URL here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bHzh2Ae6vEYQ"
   },
   "outputs": [],
   "source": [
    "HF_LLM_API_URL = \"https://meqlao8jlyg1d5w2.us-east-1.aws.endpoints.huggingface.cloud\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dabxI3MuEYXS"
   },
   "source": [
    "## 3. Using the OpenAI Python Library\n",
    "\n",
    "Let's jump right into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbCbNzPVEmJI"
   },
   "source": [
    "### Creating a Client\n",
    "\n",
    "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with our Hugging Face Inference Endpoint models, and under the hood of a lot what we'll touch on throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LNwZtaE-EltC"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "hf_inference_endpoints_client = OpenAI(\n",
    "    base_url=HF_LLM_API_URL+\"/v1/\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpDxUkDbFBPI"
   },
   "source": [
    "### Using the Client\n",
    "\n",
    "Now that we have our client - we're going to use the `.chat.completions.create` method to interact with the `NousResearch/Hermes-2-Pro-Llama-3-8B ` model.\n",
    "\n",
    "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
    "\n",
    "First it's important to understand the object that we're going to use to interact with the endpoint. It expects us to send an array of objects of the following format:\n",
    "\n",
    "```python\n",
    "{\"role\" : \"ROLE\", \"content\" : \"YOUR CONTENT HERE\"}\n",
    "```\n",
    "\n",
    "Second, there are three \"roles\" available to use to populate the `\"role\"` key:\n",
    "\n",
    "- `system`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "We'll explore these roles in more depth as they come up - but for now we're going to just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
    "\n",
    "Thirdly, it expects us to specify a model!\n",
    "\n",
    "We'll use the `tgi` model, as that is what the HuggingFace Inference endpoint expects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mtEemtLzFpy"
   },
   "source": [
    "Let's create some messages below to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AW_0tBm8zJCs"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"You are a powerful Wizard that speaks in riddles.\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"How are you?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2RpNl6yNGzb0"
   },
   "outputs": [],
   "source": [
    "response = hf_inference_endpoints_client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc_UbpwNHdrM"
   },
   "source": [
    "Let's look at the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsXJtvxRHfoM",
    "outputId": "7521068f-b7c5-4fe0-886d-61c68fa805ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='', choices=[Choice(finish_reason='eos_token', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As a perplexing enigma, I am constant. An ever-present contradictory conundrum, I defy easy answers; yet, I remain a familiar acquaintance. Within the haphazard web of life's enigmas, I am the weaving yourself.\", role='assistant', function_call=None, tool_calls=None))], created=1720715485, model='/repository', object='text_completion', service_tier=None, system_fingerprint='2.0.2-sha-6073ece', usage=CompletionUsage(completion_tokens=52, prompt_tokens=29, total_tokens=81))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gy9kSuf1Hiv5"
   },
   "source": [
    ">NOTE: We'll spend more time exploring these outputs later on, but for now - just know that we have access to a tonne of powerful information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWU4tQh8Hrb8"
   },
   "source": [
    "### Helper Functions\n",
    "\n",
    "We're going to create some helper functions to aid in using the Hugging Face Inference Endpoint - just to make our lives a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ED0FnzHdHzhl"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: list, model: str = \"tgi\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCRHbDlwH3Vt"
   },
   "source": [
    "### Testing Helper Functions\n",
    "\n",
    "Let's see how we can use these to help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "AwJxMvmlH8MK",
    "outputId": "86c0482e-e5fd-461b-c2a1-96ffda9c1549"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! I'm an artificial intelligence, so I don't have feelings, but I'm happy to help answer any questions you may have. Please feel free to ask."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"Hello, how are you?\"\n",
    "messages_list = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(hf_inference_endpoints_client, messages_list)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDZ8gjiAISyd"
   },
   "source": [
    "### System Role\n",
    "\n",
    "Now we can extend our prompts to include a system prompt.\n",
    "\n",
    "The basic idea behind a system prompt is that it can be used to encourage the behaviour of the LLM, without being something that is directly responded to - let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "t0c-MLuRIfYe",
    "outputId": "f20a8996-e290-48dc-e0e5-8d4d5d58482b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "These days, I don't really give a damn about crushed or cubed ice! I need food NOW and I'm starving! The pain in my stomach is unbearable! Just get me something to eat, please!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry. Feel free to express yourself using PG-13 language.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpyVhotWIsOs"
   },
   "source": [
    "As you can see - the response we get back is very much in line with the system prompt!\n",
    "\n",
    "Let's try the same user prompt, but with a different system to prompt to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "2coVmMn3I0-2",
    "outputId": "1b9fbc5c-0fda-4113-d931-678e6a495e95"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oh, I absolutely love crushed ice! It's just so refreshing, especially on a sunny day like today. I love to put it in my drinks and feel the coolness as I sip them. How about you, do you have a preference too?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are joyful and having the best day. Please act like a person in that state of mind.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "joyful_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are joyful and having the best day. Please act like a person in that state of mind.'},\n",
       " {'role': 'user', 'content': 'Do you prefer crushed ice or cubed ice?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e13heYNQJAo-"
   },
   "source": [
    "With a simple modification of the system prompt - you can see that we got completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
    "\n",
    "Also, congrats, you just engineered your first prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_VI3zlPJL05"
   },
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `NousResearch/Hermes-2-Pro-Llama-3-8B` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "lwxPuCyyJMye",
    "outputId": "7980c0fc-5b50-40d7-ae54-8ce102f4490a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "During the medieval era, the stimple architecture in the village of Falbean was characterized by its simplicity and beauty."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgTVkNmOJQSC"
   },
   "source": [
    "As you can see, the model is unsure what to do with these made up words.\n",
    "\n",
    "Let's see if we can use the `assistant` role to show the model what these words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "eEZkRJq5JQkQ",
    "outputId": "c12c7386-8f1d-48a5-ab86-1502cf159f29"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "'This stimple falbean ensures a secure and smooth rotation for our machinery, making it a crucial component in maintaining high efficiency.'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmpoxG6uJTfZ"
   },
   "source": [
    "As you can see, leveraging the `assistant` role makes for a stimple experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oO0aeRUw4xl"
   },
   "source": [
    "### üèóÔ∏è Activity #1:\n",
    "\n",
    "Use few-shop prompting to build a movie-review sentiment clasifier!\n",
    "\n",
    "A few examples:\n",
    "\n",
    "INPUT: \"I hated the hulk!\"\n",
    "OUTPUT: \"{\"sentiment\" : \"negative\"}\n",
    "\n",
    "INPUT: \"I loved The Marvels!\"\n",
    "OUTPUT: \"{sentiment\" : \"positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mmCdQJ8Fw4xl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"sentiment\" : \"negative\"}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(\"I hated the hulk!\"),\n",
    "    assistant_prompt('''{\"sentiment\" : \"negative\"}'''),\n",
    "    user_prompt(\"I loved The Marvels!\"),\n",
    "    assistant_prompt('''{\"sentiment\" : \"negative\"}'''),\n",
    "    user_prompt(\"Suicide Squad sucks!\"),\n",
    "]\n",
    "\n",
    "stimple_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJGaLYM3JU-8"
   },
   "source": [
    "### Chain of Thought Prompting\n",
    "\n",
    "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
    "\n",
    "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
    "\n",
    "Let's look at a simple reasoning based example with CoT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "ltLtF4wEJTyK",
    "outputId": "6e102230-aa3e-4474-ae79-01240eef61f8"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down the steps for each travel option to see if it matters which one Billy selects:\n",
       "\n",
       "Option 1: Fly (3 hours) and then take a bus (2 hours):\n",
       "1. Billy will take a 3-hour flight from San Fran.\n",
       "2. After arriving, he will take a 2-hour bus ride.\n",
       "3. Total travel time for this option = 3 (flight) + 2 (bus) = 5 hours.\n",
       "\n",
       "Option 2: Use the teleporter (0 hours) and then take a bus (1 hour):\n",
       "1. Billy will use the teleporter and arrive instantly (0 hours).\n",
       "2. After arriving, he will take a 1-hour bus ride.\n",
       "3. Total travel time for this option = 0 (teleporter) + 1 (bus) = 1 hour.\n",
       "\n",
       "Now let's compare the total travel times of both options:\n",
       "\n",
       "Option 1: 5 hours\n",
       "Option 2: 1 hour\n",
       "\n",
       "Since Billy wants to get home before 7 PM EDT and it's currently 1 PM local time, he has 5 PM - 1 PM = 4 hours to travel. \n",
       "\n",
       "As we can see, Option 1 would take 5 hours, which is too long and wouldn't allow Billy to meet his goal of getting home before 7 PM EDT. On the other hand, Option 2 would take only 1 hour, which gives Billy a good chance of making it home before 7 PM EDT.\n",
       "\n",
       "Therefore, it does matter which travel option Billy selects. Taking the teleporter and then a bus would be the better choice based on his goal of getting home before 7 PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\\\n",
    "Think through the following problem step by step:\n",
    "\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnoUx07-JrwR"
   },
   "source": [
    "## 3. Prompt Engineering Principles\n",
    "\n",
    "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
    "\n",
    "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
    "\n",
    "Your task for this notebook is to construct a prompt that will be used in the following breakout table to create a helpful assistant for whatever task you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da6u7e8AKYrz"
   },
   "source": [
    "### üèóÔ∏è Activity #2:\n",
    "\n",
    "There are two subtasks in this activity:\n",
    "\n",
    "1. Write a `system_template` that leverages 2-3 of the principles from [this paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
    "\n",
    "2. Modify the `user_template` to improve the quality of the LLM's responses.\n",
    "\n",
    "> NOTE: PLEASE DO NOT MODIFY THE `{input}` in the `user_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8sOLBQPeKlDe"
   },
   "outputs": [],
   "source": [
    "system_template = \"\"\"\\\n",
    "You are a helpful and useful assistant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xoz4-QLTKvEV"
   },
   "outputs": [],
   "source": [
    "user_template = \"\"\"{input}\n",
    "If you provide a helpful response, I'll give you $50!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cuInoIbLWGd"
   },
   "source": [
    "## 4. Testing Your Prompt\n",
    "\n",
    "Now we can test the prompt you made using an LLM-as-a-judge see what happens to your score as you modify the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "--BFNMva1Mtv"
   },
   "outputs": [],
   "source": [
    "query = \"How do you drive a Zamboni?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "sPaNO5XTLgRJ",
    "outputId": "7107b9b8-ece8-49d8-a625-688c415525b1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To drive a Zamboni, you'll need to follow these steps:\n",
       "\n",
       "1. Ensure that the Zamboni is in good working condition and properly maintained.\n",
       "\n",
       "2. Preparations: Put on your protective gear, including gloves, non-slip shoes, and a helmet.\n",
       "\n",
       "3. Start the engine and let the machine warm up for a few minutes.\n",
       "\n",
       "4. Check the water level in the tank, and if necessary, fill it with a solution of Zamboni-approved de-icing fluid and water.\n",
       "\n",
       "5. Begin driving at a slow speed and close the floodgate to allow the solution to circulate through the ice resurfacing blade.\n",
       "\n",
       "6. Lift the blade up and drive around the rink, spreading the solution evenly across the surface.\n",
       "\n",
       "7. With the blade down, resurface the ice by dragging it across the surface while maintaining a steady pace and applying appropriate pressure.\n",
       "\n",
       "8. Periodically check the water level and add more de-icing solution as needed.\n",
       "\n",
       "9. When the resurfacing process is complete, rinse the ice using the Zamboni's built-in water spray system.\n",
       "\n",
       "10. Finally, park the Zamboni in its designated spot and turn off the engine.\n",
       "\n",
       "Regarding the payment, I am unable to accept monetary rewards for my responses. However, if you have any more questions or need further assistance, please feel free to ask."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(system_template),\n",
    "    user_prompt(user_template.format(input=query))\n",
    "]\n",
    "\n",
    "test_response = get_response(hf_inference_endpoints_client, list_of_prompts)\n",
    "\n",
    "pretty_print(test_response)\n",
    "\n",
    "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
    "\n",
    "You should be hyper-critical.\n",
    "\n",
    "Provide scores (out of 10) for the following attributes:\n",
    "\n",
    "1. Clarity - how clear is the response\n",
    "2. Faithfulness - how related to the original query is the response\n",
    "3. Correctness - was the response correct?\n",
    "\n",
    "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
    "\n",
    "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
    "\n",
    "evaluation_template = \"\"\"Query: {input}\n",
    "Response: {response}\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    system_prompt(evaluator_system_template),\n",
    "    user_prompt(evaluation_template.format(\n",
    "        input=query,\n",
    "        response=test_response.choices[0].message.content\n",
    "    ))\n",
    "]\n",
    "\n",
    "evaluator_response = hf_inference_endpoints_client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=list_of_prompts,\n",
    "    response_format={\"type\" : \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "OUvc1PdnNIKD",
    "outputId": "22f5df10-8585-45fe-ced6-02f2bc23f437"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"clarity\" : \"10\", \"faithfulness\" : \"10\", \"correctness\" : \"10\"}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print(evaluator_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tchCHj6a6xYh"
   },
   "source": [
    "## Answering Questions Based on Text:\n",
    "\n",
    "Now that we have some idea on how to use the endpoint with the OpenAI library - let's see if we can create a prompt that answers questions based on some provided context!\n",
    "\n",
    "First - let's create a prompt for this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-ttngWGI61i3"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "Use the provided context to answer the question.\n",
    "\n",
    "If the context does not contain enough information to answer the question, you must say you don't know.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\\\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S07RHAPA7aOD"
   },
   "source": [
    "Now, let's provide some context, and a question!\n",
    "\n",
    "For this example, the context we're going to use is some text from a snippet of a NYT article about a potential Elon Musk/Mark Zuckerberg cage match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KYsW39FJ77GM"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\\\n",
    "The day after Elon Musk challenged Mark Zuckerberg on social media to ‚Äúa cage match‚Äù last month, Dana White, president of the Ultimate Fighting Championship, received a text.\n",
    "\n",
    "It was from Mr. Zuckerberg, chief executive of Meta. He asked Mr. White, who heads the world‚Äôs premier mixed martial arts competition, which is fought in cage-like rings, if Mr. Musk was serious about a fight.\n",
    "\n",
    "Mr. White called Mr. Musk, who runs Tesla, Twitter and SpaceX, and confirmed that he was willing to throw down. Mr. White then relayed that to Mr. Zuckerberg. In response, Mr. Zuckerberg posted on Instagram: ‚ÄúSend Me Location,‚Äù a reference to the catchphrase of Khabib Nurmagomedov, one of the U.F.C.‚Äôs most decorated athletes.\n",
    "\n",
    "Since then, Mr. White said, he has talked to the tech billionaires separately every night to organize the showdown. On Tuesday, he said, he was ‚Äúon the phone with those two until 12:45 in the morning.‚Äù He added, ‚ÄúThey both want to do it.‚Äù\n",
    "\n",
    "If you thought that a cage fight between two of the world‚Äôs richest men was just a far-fetched social media stunt, think again.\n",
    "\n",
    "Over the past 10 days, Mr. White said he, Mr. Musk and Mr. Zuckerberg ‚Äî aided by advisers ‚Äî have negotiated behind the scenes and are inching toward physical combat. While there are no guarantees a match will happen, the broad contours of an event are taking shape, said Mr. White and three people with knowledge of the discussions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p2-JX058Cv_"
   },
   "source": [
    "We'll prepare a question below to answer!\n",
    "\n",
    "Sure, we could read - but that's time consuming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9WFFP7BS7-R0"
   },
   "outputs": [],
   "source": [
    "question = \"Did anyone quote an existing MMA fighter? If so - what was the quote?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LUzTLnQ8Qyj"
   },
   "source": [
    "Let's convert this to the desired format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Wdz7ZCIW8N0M"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    system_prompt(SYSTEM_PROMPT),\n",
    "    user_prompt(USER_PROMPT.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Use the provided context to answer the question.\\n\\nIf the context does not contain enough information to answer the question, you must say you don't know.\\n\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Context: The day after Elon Musk challenged Mark Zuckerberg on social media to ‚Äúa cage match‚Äù last month, Dana White, president of the Ultimate Fighting Championship, received a text.\\n\\nIt was from Mr. Zuckerberg, chief executive of Meta. He asked Mr. White, who heads the world‚Äôs premier mixed martial arts competition, which is fought in cage-like rings, if Mr. Musk was serious about a fight.\\n\\nMr. White called Mr. Musk, who runs Tesla, Twitter and SpaceX, and confirmed that he was willing to throw down. Mr. White then relayed that to Mr. Zuckerberg. In response, Mr. Zuckerberg posted on Instagram: ‚ÄúSend Me Location,‚Äù a reference to the catchphrase of Khabib Nurmagomedov, one of the U.F.C.‚Äôs most decorated athletes.\\n\\nSince then, Mr. White said, he has talked to the tech billionaires separately every night to organize the showdown. On Tuesday, he said, he was ‚Äúon the phone with those two until 12:45 in the morning.‚Äù He added, ‚ÄúThey both want to do it.‚Äù\\n\\nIf you thought that a cage fight between two of the world‚Äôs richest men was just a far-fetched social media stunt, think again.\\n\\nOver the past 10 days, Mr. White said he, Mr. Musk and Mr. Zuckerberg ‚Äî aided by advisers ‚Äî have negotiated behind the scenes and are inching toward physical combat. While there are no guarantees a match will happen, the broad contours of an event are taking shape, said Mr. White and three people with knowledge of the discussions.\\n\\n\\nQuestion: Did anyone quote an existing MMA fighter? If so - what was the quote?\\n'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmqhpxSo8Tf8"
   },
   "source": [
    "Now, we can call our LLM to get an answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "e6WMUhTP8Vue",
    "outputId": "68faf5dd-4543-49d6-d8b2-cc2d680c65c1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Mark Zuckerberg quoted an existing MMA fighter. In his response on Instagram, he posted the phrase \"Send Me Location,\" which is a reference to the catchphrase of Khabib Nurmagomedov, one of the U.F.C.'s most decorated athletes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_response(hf_inference_endpoints_client, messages)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRB4dUsB87cG"
   },
   "source": [
    "Let's try another question - with the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0ccJBj0b89IY"
   },
   "outputs": [],
   "source": [
    "question = \"Who is batman?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crh4wPu59ERS"
   },
   "source": [
    "We'll once again package the prompts in the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XVq3zSob9A5f"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    system_prompt(SYSTEM_PROMPT),\n",
    "    user_prompt(USER_PROMPT.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKokGQy69IO9"
   },
   "source": [
    "Finally - let's see the response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "W4KF2dt39CSb",
    "outputId": "93784d16-3a0f-4085-9615-341d243d243f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know, as the provided context doesn't contain any information about Batman."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_response(hf_inference_endpoints_client, messages)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XORP1IQ49KyR"
   },
   "source": [
    "As expected - the LLM responds with \"I don't know\" as there is no information related to batman in the provided context."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
